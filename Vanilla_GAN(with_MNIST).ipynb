{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vanilla GAN(with MNIST).ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0T70NfjVI6BQwoPOpr63k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sangyups/VanillaGAN/blob/main/Vanilla_GAN(with_MNIST).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4B2u9fOsR_nO",
        "outputId": "184fbb5d-13f9-446e-d7f2-e1de0cd6f11d"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms, utils\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "# Training settings\n",
        "batch_size = 32\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(f'Training MNIST Model on {device}\\n{\"=\" * 44}')\n",
        "\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "print(train_dataset)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training MNIST Model on cuda\n",
            "============================================\n",
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: ./mnist_data/\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPfAzzSOSHC3"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.n_features = 128\n",
        "        self.n_out = 784\n",
        "        self.fc0 = nn.Sequential(\n",
        "            nn.Linear(self.n_features, 256),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(1024, self.n_out),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.fc0(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.n_in = 784\n",
        "        self.n_out = 1\n",
        "        self.fc0 = nn.Sequential(\n",
        "            nn.Linear(self.n_in, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "        self.fc3 = nn.Sequential(\n",
        "            nn.Linear(256, self.n_out),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = self.fc0(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOKl15Gn0R1r"
      },
      "source": [
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "loss = torch.nn.BCELoss()\n",
        "\n",
        "optimizer_G = optim.Adam(G.parameters(), lr=1e-4)\n",
        "optimizer_D = optim.Adam(D.parameters(), lr=1e-4)\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PHtwOrd26g8",
        "outputId": "097376be-c455-46b5-d537-b68880d835b1"
      },
      "source": [
        "for epoch in range(100):\n",
        "\n",
        "    # train\n",
        "    G.train()\n",
        "    D.train()\n",
        "    i = 1\n",
        "    for real_data, target in train_loader:\n",
        "        # print(i)\n",
        "        real_data = real_data.to(device)\n",
        "        real_label = torch.ones(real_data.shape[0], 1).to(device)\n",
        "        fake_label = torch.zeros(real_data.shape[0], 1).to(device)\n",
        "        noise = torch.randn(real_data.shape[0], 128).to(device)\n",
        "        fake_data = G(noise)\n",
        "        output = D(fake_data)\n",
        "        loss_g = loss(output, real_label)\n",
        "        optimizer_G.zero_grad()\n",
        "        loss_g.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        fake_data = fake_data.detach()\n",
        "\n",
        "        output_real = D(real_data)\n",
        "        loss_d_real = loss(output_real, real_label)\n",
        "        output_fake = D(fake_data)\n",
        "        loss_d_fake = loss(output_fake, fake_label)\n",
        "        loss_d_final = loss_d_real + loss_d_fake\n",
        "        optimizer_D.zero_grad()\n",
        "        loss_d_final.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "    print(\"===========epoch:\",epoch,\"===========\")\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        fake_img = fake_data.reshape([batch_size, 1, 28, 28])\n",
        "        img_grid = utils.make_grid(fake_img, nrow=10, normalize=True)\n",
        "        utils.save_image(img_grid, \"/content/gdrive/My Drive/Colab Notebooks/VanillaGAN_result/%d.png\"%(epoch+1))\n",
        "        print(\"image saved at epoch: \", epoch)\n",
        "        \n",
        "    \n",
        "    # test\n",
        "    # G.eval()\n",
        "    # D.eval()\n",
        "    # test_G_loss = 0\n",
        "    # test_D_loss = 0\n",
        "    # correct_real = 0\n",
        "    # correct_fake = 0\n",
        "    # for real_data, target in test_loader:\n",
        "    #     real_data = real_data.to(device)\n",
        "    #     real_label = torch.ones(real_data.shape[0], 1).to(device)\n",
        "    #     fake_label = torch.zeros(real_data.shape[0], 1).to(device)\n",
        "\n",
        "    #     noise = torch.randn(real_data.shape[0], 128).to(device)\n",
        "    #     fake_data = G(noise)\n",
        "    #     output = D(fake_data)\n",
        "    #     test_G_loss += loss(output, real_label).item()\n",
        "\n",
        "    #     fake_data = fake_data.detach()\n",
        "\n",
        "    #     test_output_real = D(real_data)\n",
        "    #     test_loss_d_real = loss(test_output_real, real_label)\n",
        "    #     test_output_fake = D(fake_data)\n",
        "    #     test_loss_d_fake = loss(test_output_fake, fake_label)\n",
        "    #     test_loss_d_final = test_loss_d_real + test_loss_d_fake\n",
        "    #     test_D_loss += test_loss_d_final\n",
        "    #     correct_real += (test_output_real > 0.5).sum().item()\n",
        "    #     correct_fake += (test_output_fake <= 0.5).sum().item()\n",
        "\n",
        "    # test_G_loss /= len(test_loader.dataset)\n",
        "    # test_D_loss /= len(test_loader.dataset)\n",
        "    # print(\"============epoch: \",epoch,\"==========\")\n",
        "    # print(\"Generator Loss:\", loss_g.item())\n",
        "    # print(\"Discriminator Loss:\", loss_d_final.item())\n",
        "    # print(f\"Test set: Average Generator loss: {test_G_loss}, Average Discrminator loss: {test_D_loss}\")\n",
        "    # print(f\"Accuracy for real image: {correct_real}/{len(test_loader.dataset)} ({100. * correct_real / len(test_loader.dataset):.0f}%)\")\n",
        "    # print(f\"Accuracy for fake image: {correct_fake}/{len(test_loader.dataset)} ({100. * correct_fake / len(test_loader.dataset):.0f}%)\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===========epoch: 0 ===========\n",
            "===========epoch: 1 ===========\n",
            "===========epoch: 2 ===========\n",
            "===========epoch: 3 ===========\n",
            "===========epoch: 4 ===========\n",
            "===========epoch: 5 ===========\n",
            "===========epoch: 6 ===========\n",
            "===========epoch: 7 ===========\n",
            "===========epoch: 8 ===========\n",
            "===========epoch: 9 ===========\n",
            "image saved at epoch:  9\n",
            "===========epoch: 10 ===========\n",
            "===========epoch: 11 ===========\n",
            "===========epoch: 12 ===========\n",
            "===========epoch: 13 ===========\n",
            "===========epoch: 14 ===========\n",
            "===========epoch: 15 ===========\n",
            "===========epoch: 16 ===========\n",
            "===========epoch: 17 ===========\n",
            "===========epoch: 18 ===========\n",
            "===========epoch: 19 ===========\n",
            "image saved at epoch:  19\n",
            "===========epoch: 20 ===========\n",
            "===========epoch: 21 ===========\n",
            "===========epoch: 22 ===========\n",
            "===========epoch: 23 ===========\n",
            "===========epoch: 24 ===========\n",
            "===========epoch: 25 ===========\n",
            "===========epoch: 26 ===========\n",
            "===========epoch: 27 ===========\n",
            "===========epoch: 28 ===========\n",
            "===========epoch: 29 ===========\n",
            "image saved at epoch:  29\n",
            "===========epoch: 30 ===========\n",
            "===========epoch: 31 ===========\n",
            "===========epoch: 32 ===========\n",
            "===========epoch: 33 ===========\n",
            "===========epoch: 34 ===========\n",
            "===========epoch: 35 ===========\n",
            "===========epoch: 36 ===========\n",
            "===========epoch: 37 ===========\n",
            "===========epoch: 38 ===========\n",
            "===========epoch: 39 ===========\n",
            "image saved at epoch:  39\n",
            "===========epoch: 40 ===========\n",
            "===========epoch: 41 ===========\n",
            "===========epoch: 42 ===========\n",
            "===========epoch: 43 ===========\n",
            "===========epoch: 44 ===========\n",
            "===========epoch: 45 ===========\n",
            "===========epoch: 46 ===========\n",
            "===========epoch: 47 ===========\n",
            "===========epoch: 48 ===========\n",
            "===========epoch: 49 ===========\n",
            "image saved at epoch:  49\n",
            "===========epoch: 50 ===========\n",
            "===========epoch: 51 ===========\n",
            "===========epoch: 52 ===========\n",
            "===========epoch: 53 ===========\n",
            "===========epoch: 54 ===========\n",
            "===========epoch: 55 ===========\n",
            "===========epoch: 56 ===========\n",
            "===========epoch: 57 ===========\n",
            "===========epoch: 58 ===========\n",
            "===========epoch: 59 ===========\n",
            "image saved at epoch:  59\n",
            "===========epoch: 60 ===========\n",
            "===========epoch: 61 ===========\n",
            "===========epoch: 62 ===========\n",
            "===========epoch: 63 ===========\n",
            "===========epoch: 64 ===========\n",
            "===========epoch: 65 ===========\n",
            "===========epoch: 66 ===========\n",
            "===========epoch: 67 ===========\n",
            "===========epoch: 68 ===========\n",
            "===========epoch: 69 ===========\n",
            "image saved at epoch:  69\n",
            "===========epoch: 70 ===========\n",
            "===========epoch: 71 ===========\n",
            "===========epoch: 72 ===========\n",
            "===========epoch: 73 ===========\n",
            "===========epoch: 74 ===========\n",
            "===========epoch: 75 ===========\n",
            "===========epoch: 76 ===========\n",
            "===========epoch: 77 ===========\n",
            "===========epoch: 78 ===========\n",
            "===========epoch: 79 ===========\n",
            "image saved at epoch:  79\n",
            "===========epoch: 80 ===========\n",
            "===========epoch: 81 ===========\n",
            "===========epoch: 82 ===========\n",
            "===========epoch: 83 ===========\n",
            "===========epoch: 84 ===========\n",
            "===========epoch: 85 ===========\n",
            "===========epoch: 86 ===========\n",
            "===========epoch: 87 ===========\n",
            "===========epoch: 88 ===========\n",
            "===========epoch: 89 ===========\n",
            "image saved at epoch:  89\n",
            "===========epoch: 90 ===========\n",
            "===========epoch: 91 ===========\n",
            "===========epoch: 92 ===========\n",
            "===========epoch: 93 ===========\n",
            "===========epoch: 94 ===========\n",
            "===========epoch: 95 ===========\n",
            "===========epoch: 96 ===========\n",
            "===========epoch: 97 ===========\n",
            "===========epoch: 98 ===========\n",
            "===========epoch: 99 ===========\n",
            "image saved at epoch:  99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyYA5RJNriHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daba272e-1736-45b8-e448-d78dda11a486"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4azmuqEpU76f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}